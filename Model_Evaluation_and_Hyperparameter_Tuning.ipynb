{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj9vC8FR2dPC",
        "outputId": "0c376705-ac5b-4b97-bdb6-5418af2d9aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Performance:\n",
            "                  Model  Accuracy  Precision    Recall  F1 Score\n",
            "0  Logistic Regression  0.797203   0.854167  0.650794  0.738739\n",
            "1        Random Forest  0.776224   0.771930  0.698413  0.733333\n",
            "2                  SVM  0.636364   0.648649  0.380952  0.480000\n",
            "\n",
            "Final Model Comparison:\n",
            "                    Model  Accuracy  Precision    Recall  F1 Score\n",
            "3  Random Forest (Tuned)  0.804196   0.830189  0.698413  0.758621\n",
            "0    Logistic Regression  0.797203   0.854167  0.650794  0.738739\n",
            "1          Random Forest  0.776224   0.771930  0.698413  0.733333\n",
            "4            SVM (Tuned)  0.748252   0.764706  0.619048  0.684211\n",
            "2                    SVM  0.636364   0.648649  0.380952  0.480000\n",
            "\n",
            " Best Model: Random Forest (Tuned) with F1 Score = 0.7586\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loading the Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Basic preprocessing\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "df.dropna(inplace=True)\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "# Step 2: Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Defining the  models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC()}\n",
        "\n",
        "# Step 4: Train and Evaluate Models\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results.append({'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Initial Model Performance:\\n\", results_df)\n",
        "\n",
        "# Step 5: Hyperparameter Tuning\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# GridSearchCV for RandomForest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [4, 6, 8, None],\n",
        "    'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "# RandomizedSearchCV for SVC\n",
        "param_dist_svc = {'C': [0.1, 1, 10, 100],\n",
        "                  'kernel': ['linear', 'rbf'],\n",
        "                  'gamma': ['scale', 'auto']}\n",
        "\n",
        "random_svc = RandomizedSearchCV(SVC(), param_distributions=param_dist_svc, n_iter=10, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
        "random_svc.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluating the tuned models\n",
        "tuned_models = {\n",
        "    'Random Forest (Tuned)': grid_rf.best_estimator_,\n",
        "    'SVM (Tuned)': random_svc.best_estimator_\n",
        "}\n",
        "\n",
        "for name, model in tuned_models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results.append({'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1})\n",
        "\n",
        "\n",
        "final_results_df = pd.DataFrame(results)\n",
        "print(\"\\nFinal Model Comparison:\\n\", final_results_df.sort_values(by=\"F1 Score\", ascending=False))\n",
        "\n",
        "best_model = final_results_df.sort_values(by='F1 Score', ascending=False).iloc[0]\n",
        "print(f\"\\n Best Model: {best_model['Model']} with F1 Score = {best_model['F1 Score']:.4f}\")\n"
      ]
    }
  ]
}